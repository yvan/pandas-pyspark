{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249a99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8bf22",
   "metadata": {},
   "source": [
    "First we will setup a spark context, load our csvs from the filesystem (whether it's just our local example from the pandas pipe or a giant dataset stored in hdfs). We will merge the two files into one dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca12554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/23 11:16:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "s = SparkSession.builder.appName(\"my_app_name\").getOrCreate()\n",
    "s.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4229a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = s.read.csv('data1.csv', header=True, inferSchema=True)\n",
    "df2 = s.read.csv('data2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd31d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df = df.withColumn(\"index\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00398b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 13)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ba4cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw| sYef|00L3k|       OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|     null|     74|       94|        7|  89|    96|     1|    93|       1|abc23| 50.0|2020-02-01|\n",
      "|     null|     90|       68|       94|  72|    32|    81|    58|       0|abc23| 50.0|2020-02-01|\n",
      "|     null|     79|       10|       99|  39|    76|    28|    44|       1|  nan| 50.0|2020-02-01|\n",
      "|     null|      5|       83|       78|  26|    96|    79|    66|       0|   81| 50.0|2020-02-01|\n",
      "|     null|     66|       89|       70|  17|    43|    95|    89|       1|   75| 50.0|2020-02-01|\n",
      "|     null|     44|       89|       85|  87|    90|    45|    72|       0|abc23| 50.0|2020-02-01|\n",
      "|     null|     36|       94|       28|   6|    75|    54|    18|       0| null| 10.0|2020-02-01|\n",
      "|     null|     49|       94|       98|  24|    15|    10|    40|       0|abc23| 50.0|2020-02-01|\n",
      "|     null|      8|       16|       51|  57|    11|    47|    54|       0|   99| 50.0|2020-02-01|\n",
      "|     null|     33|       67|       98|   1|    40|    16|    87|       0|   34| 50.0|2020-02-01|\n",
      "|     null|     89|       84|       71|  12|    36|    23|    41|       1|  nan| 10.0|2020-02-01|\n",
      "|     null|     52|       89|       13|  87|    35|    56|    11|       1|  nan| 10.0|2020-02-01|\n",
      "|     null|     76|        6|       60|  67|    89|    53|     1|       1|   37| 50.0|2020-02-01|\n",
      "|     null|     87|       62|       53|  91|    27|    30|    69|       0|   79| 10.0|2020-02-01|\n",
      "|     null|     43|       15|       12|  56|    91|    87|    51|       1|   40| 50.0|2020-02-01|\n",
      "|     null|     60|        5|       70|  55|    12|    61|    69|       1|   97| 10.0|2020-02-01|\n",
      "|     null|     16|       34|       11|  38|    24|    13|    11|       1|abc23| 50.0|2020-02-01|\n",
      "|     null|     94|       20|       98|  63|    17|    73|     4|       0|abc23| 50.0|2020-02-01|\n",
      "|     null|     21|       18|       49|  90|     8|    36|    38|       0|abc23| 50.0|2020-02-01|\n",
      "|     null|     45|        4|       79|  29|    53|    36|    11|       0|    0| 10.0|2020-02-01|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d67613a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw| sYef|00L3k|       OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|     null|     30|       57|       49|  34|    83|    26|    43|       0|  nan| 50.0|2020/02/01|\n",
      "|     null|      8|       92|       51|  37|    23|    46|    86|       1|abc23| 10.0|2020/02/01|\n",
      "|     null|     27|       65|       20|  37|    29|    73|    70|       0|abc23| 10.0|2020/02/01|\n",
      "|     null|     49|       56|       42|  88|    49|    79|    11|       1| null| 10.0|2020/02/01|\n",
      "|     null|     46|       46|       84|  75|    97|    92|    11|       1|   86| 50.0|2020/02/01|\n",
      "|     null|     11|       28|       53|  99|    77|    21|    60|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     58|       53|       38|  21|    53|    71|    52|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     67|       92|       95|  31|    89|    86|    11|       0|abc23| 50.0|2020/02/01|\n",
      "|     null|     46|        1|       93|  84|    59|    80|    31|       1|abc23| 10.0|2020/02/01|\n",
      "|     null|     67|       13|       40|  81|    21|    35|    24|       0|   13| 50.0|2020/02/01|\n",
      "|     null|     43|       63|       82|  87|    52|    29|    83|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     24|       98|       64|  38|    20|    25|    83|       0|abc23| 10.0|2020/02/01|\n",
      "|     null|      8|       54|       63|  47|     4|    35|    84|       0|abc23| 50.0|2020/02/01|\n",
      "|     null|     69|       67|       82|  30|    96|    54|    25|       1| null| 50.0|2020/02/01|\n",
      "|     null|     19|       80|       68|  95|    62|    19|    80|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     92|       38|       79|  24|    90|    40|    20|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     78|       91|       84|  16|    40|    82|    88|       1|abc23| 10.0|2020/02/01|\n",
      "|     null|     36|        7|       68|  13|     8|    69|    50|       0|abc23| 10.0|2020/02/01|\n",
      "|     null|     99|       92|       87|  82|    10|    51|    56|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     27|       97|       65|  13|    24|    62|    36|       1|  nan| 50.0|2020/02/01|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(F.desc(\"index\")).drop(\"index\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d0eb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SymxUUBCx', 'int'),\n",
       " ('RcKB398', 'int'),\n",
       " ('uNQMYeBmQ', 'int'),\n",
       " ('lOihuqh9a', 'int'),\n",
       " ('pIkq', 'int'),\n",
       " ('21rkd6', 'int'),\n",
       " ('yLdLOm', 'int'),\n",
       " ('CLrQTb', 'int'),\n",
       " ('LFThQKIw', 'int'),\n",
       " ('sYef', 'string'),\n",
       " ('00L3k', 'double'),\n",
       " ('OjQ', 'string'),\n",
       " ('index', 'bigint')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a35cb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RcKB398', 'LFThQKIw', 'sYef', '00L3k', 'OjQ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1, col2, col3, col4, col5 = df.columns[1], df.columns[8], df.columns[9], df.columns[10], df.columns[11]\n",
    "(col1, col2, col3, col4, col5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "058e6f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__and__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_asc_doc',\n",
       " '_asc_nulls_first_doc',\n",
       " '_asc_nulls_last_doc',\n",
       " '_bitwiseAND_doc',\n",
       " '_bitwiseOR_doc',\n",
       " '_bitwiseXOR_doc',\n",
       " '_contains_doc',\n",
       " '_desc_doc',\n",
       " '_desc_nulls_first_doc',\n",
       " '_desc_nulls_last_doc',\n",
       " '_endswith_doc',\n",
       " '_eqNullSafe_doc',\n",
       " '_ilike_doc',\n",
       " '_isNotNull_doc',\n",
       " '_isNull_doc',\n",
       " '_jc',\n",
       " '_like_doc',\n",
       " '_rlike_doc',\n",
       " '_startswith_doc',\n",
       " 'alias',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'astype',\n",
       " 'between',\n",
       " 'bitwiseAND',\n",
       " 'bitwiseOR',\n",
       " 'bitwiseXOR',\n",
       " 'cast',\n",
       " 'contains',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'dropFields',\n",
       " 'endswith',\n",
       " 'eqNullSafe',\n",
       " 'getField',\n",
       " 'getItem',\n",
       " 'ilike',\n",
       " 'isNotNull',\n",
       " 'isNull',\n",
       " 'isin',\n",
       " 'like',\n",
       " 'name',\n",
       " 'otherwise',\n",
       " 'over',\n",
       " 'rlike',\n",
       " 'startswith',\n",
       " 'substr',\n",
       " 'when',\n",
       " 'withField']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(F.col(col4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1d18a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1,2,3,25]\n",
    "df = df.filter(~F.col(col4).isin(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d199d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple transform\n",
    "df = df.withColumn(col1, F.col(col1)*5)\n",
    "\n",
    "# using when to set two different values \n",
    "df = df.withColumn(col2, F.when(F.col(col2) == 1, True).otherwise(False))\n",
    "\n",
    "# only casting to float if we can bring the entire column\n",
    "row_ct = df.count()\n",
    "rowswith_col3float = df.filter(F.col(col3).cast(\"float\").isNotNull()).count()\n",
    "if row_ct == rowswith_col3float:\n",
    "    df = df.withColumn(col3, F.col(col3).cast(\"float\"))\n",
    "\n",
    "# using a regex to clean/remove anything non numerical in any string,\n",
    "# and then do the cast to float for non null (empty str) values\n",
    "df = df.withColumn(col3, F.regexp_replace(F.col(col3), r\"[^0-9\\\\.]\", '').cast(\"double\"))\n",
    "\n",
    "# using a custom function to convert col4 back to int\n",
    "# note the necessary redundant logic to process null\n",
    "def floatint(x):\n",
    "    return int(float(x))\n",
    "int_udf = F.udf(lambda m: None if m is None else floatint(m))\n",
    "df = df.withColumn(col4, F.when(F.col(col4).isNotNull(), int_udf(F.col(col4))).otherwise(None))\n",
    "# df = df.withColumn(col4, int_udf(F.col(col4))) # can also just do a udf without when\n",
    "\n",
    "# same, using a regex for matching dates\n",
    "data_regex = r\"\\d{2,4}(\\.|\\-|\\/|\\\\)+\\d{2,4}(\\.|\\-|\\/|\\\\)+\\d{2,4}(\\s)*(\\d{2}\\:\\d{2}\\:\\d{2})?(\\.\\d{3})?|^$\"\n",
    "df = df.withColumn(col5, F.when(F.regexp_replace(F.col(col5), data_regex, '').isNotNull(),\\\n",
    "                                F.to_timestamp(F.col(col5), 'yyyy/MM/dd')).otherwise(None))\n",
    "# df = df.withColumn(col5, F.to_timestamp(F.col(col5), 'yyyy/MM/dd')) # simpler alternative\n",
    "\n",
    "# replace and nan in the df with nulls\n",
    "df = df.replace(np.nan, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db55630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw|sYef|00L3k| OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|       51|     10|       37|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       83|    440|       38|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       40|    150|       25|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       27|    260|       66|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       65|    215|       57|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       59|     70|       93|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       28|    420|       20|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       51|      5|       52|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       46|    180|       20|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       28|    100|       63|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       82|    465|       61|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       99|    285|       34|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       30|    145|       34|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       59|     10|       16|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       17|    125|       18|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        9|    440|        2|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       54|    315|       90|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        1|     10|       73|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        3|    355|       63|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        9|    160|       57|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1898b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw|sYef|00L3k|                OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "|     null|    150|       57|       49|  34|    83|    26|    43|   false|null|   50|2020-02-01 00:00:00|\n",
      "|     null|    410|       68|       92|  68|    43|    76|    61|    true|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|     40|       92|       51|  37|    23|    46|    86|    true|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|     65|        7|       89|  81|    52|    86|    87|   false|34.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    135|       65|       20|  37|    29|    73|    70|   false|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|    245|       56|       42|  88|    49|    79|    11|    true|null|   10|2020-02-01 00:00:00|\n",
      "|     null|    230|       46|       84|  75|    97|    92|    11|    true|86.0|   50|2020-02-01 00:00:00|\n",
      "|     null|     55|       28|       53|  99|    77|    21|    60|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|     45|       27|       89|  99|    57|    36|    59|    true|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    290|       53|       38|  21|    53|    71|    52|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    425|       67|       48|  19|    35|     1|    72|   false|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    335|       92|       95|  31|    89|    86|    11|   false|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    230|        1|       93|  84|    59|    80|    31|    true|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|    335|       13|       40|  81|    21|    35|    24|   false|13.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    215|       63|       82|  87|    52|    29|    83|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    120|       98|       64|  38|    20|    25|    83|   false|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|     40|       54|       63|  47|     4|    35|    84|   false|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    345|       67|       82|  30|    96|    54|    25|    true|null|   50|2020-02-01 00:00:00|\n",
      "|     null|     95|       80|       68|  95|    62|    19|    80|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    460|       38|       79|  24|    90|    40|    20|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the last 20 rows\n",
    "df.orderBy(F.desc(\"index\")).drop(\"index\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b388cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SymxUUBCx', 'int'),\n",
       " ('RcKB398', 'int'),\n",
       " ('uNQMYeBmQ', 'int'),\n",
       " ('lOihuqh9a', 'int'),\n",
       " ('pIkq', 'int'),\n",
       " ('21rkd6', 'int'),\n",
       " ('yLdLOm', 'int'),\n",
       " ('CLrQTb', 'int'),\n",
       " ('LFThQKIw', 'boolean'),\n",
       " ('sYef', 'double'),\n",
       " ('00L3k', 'string'),\n",
       " ('OjQ', 'timestamp'),\n",
       " ('index', 'bigint')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27faf4",
   "metadata": {},
   "source": [
    "You can see the same kinds of transforms are avaialable in pyspark and you can do most of what you'd do with a dataframe. There's also ways to do this outside the dataframe api with a raw rdd but that's beyond the scope of what I wanted to show here.\n",
    "\n",
    "# This example was simple but your data problems may be complex.\n",
    "\n",
    "# For any of your complex data problems I'm available to hire on contract to help you build and scale whatever data decision engine you need for your business. \n",
    "\n",
    "# Reach out by messaging inquire@automatedinnovations.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a5990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
