{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249a99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8bf22",
   "metadata": {},
   "source": [
    "First we will setup a spark context, load our csvs from the filesystem (whether it's just our local example from the pandas pipe or a giant dataset stored in hdfs). We will merge the two files into one dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca12554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 16:27:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "s = SparkSession.builder.appName(\"my_app_name\").getOrCreate()\n",
    "s.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4229a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = s.read.csv('data1.csv', header=True, inferSchema=True)\n",
    "df2 = s.read.csv('data2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd31d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df = df.withColumn(\"index\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00398b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 13)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba4cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw|sYef|00L3k| OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|       51|      2|       37|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       83|     88|       38|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       40|     30|       25|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       27|     52|       66|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       65|     43|       57|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       59|     14|       93|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       28|     84|       20|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       51|      1|       52|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       46|     36|       20|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       28|     20|       63|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       82|     93|       61|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       99|     57|       34|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       30|     29|       34|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       59|      2|       16|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       17|     25|       18|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|        9|     88|        2|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|       54|     63|       90|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|        1|      2|       73|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|        3|     71|       63|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "|        9|     32|       57|     null|null|  null|  null|  null|    null|null| null|null|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67613a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw| sYef|00L3k|       OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "|     null|     30|       57|       49|  34|    83|    26|    43|       0|  nan| 50.0|2020/02/01|\n",
      "|     null|     82|       68|       92|  68|    43|    76|    61|       1|abc23| 25.0|2020/02/01|\n",
      "|     null|      8|       92|       51|  37|    23|    46|    86|       1|abc23| 10.0|2020/02/01|\n",
      "|     null|     13|        7|       89|  81|    52|    86|    87|       0|   34| 25.0|2020/02/01|\n",
      "|     null|     27|       65|       20|  37|    29|    73|    70|       0|abc23| 10.0|2020/02/01|\n",
      "|     null|     49|       56|       42|  88|    49|    79|    11|       1| null| 10.0|2020/02/01|\n",
      "|     null|     46|       46|       84|  75|    97|    92|    11|       1|   86| 50.0|2020/02/01|\n",
      "|     null|     11|       28|       53|  99|    77|    21|    60|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|      9|       27|       89|  99|    57|    36|    59|       1|abc23| 25.0|2020/02/01|\n",
      "|     null|     58|       53|       38|  21|    53|    71|    52|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     85|       67|       48|  19|    35|     1|    72|       0|abc23| 25.0|2020/02/01|\n",
      "|     null|     67|       92|       95|  31|    89|    86|    11|       0|abc23| 50.0|2020/02/01|\n",
      "|     null|     46|        1|       93|  84|    59|    80|    31|       1|abc23| 10.0|2020/02/01|\n",
      "|     null|     67|       13|       40|  81|    21|    35|    24|       0|   13| 50.0|2020/02/01|\n",
      "|     null|     43|       63|       82|  87|    52|    29|    83|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     24|       98|       64|  38|    20|    25|    83|       0|abc23| 10.0|2020/02/01|\n",
      "|     null|      8|       54|       63|  47|     4|    35|    84|       0|abc23| 50.0|2020/02/01|\n",
      "|     null|     69|       67|       82|  30|    96|    54|    25|       1| null| 50.0|2020/02/01|\n",
      "|     null|     19|       80|       68|  95|    62|    19|    80|       1|abc23| 50.0|2020/02/01|\n",
      "|     null|     92|       38|       79|  24|    90|    40|    20|       1|abc23| 50.0|2020/02/01|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(F.desc(\"index\")).drop(\"index\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d0eb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SymxUUBCx', 'int'),\n",
       " ('RcKB398', 'int'),\n",
       " ('uNQMYeBmQ', 'int'),\n",
       " ('lOihuqh9a', 'int'),\n",
       " ('pIkq', 'int'),\n",
       " ('21rkd6', 'int'),\n",
       " ('yLdLOm', 'int'),\n",
       " ('CLrQTb', 'int'),\n",
       " ('LFThQKIw', 'int'),\n",
       " ('sYef', 'string'),\n",
       " ('00L3k', 'double'),\n",
       " ('OjQ', 'string'),\n",
       " ('index', 'bigint')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a35cb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RcKB398', 'LFThQKIw', 'sYef', '00L3k', 'OjQ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1, col2, col3, col4, col5 = df.columns[1], df.columns[8], df.columns[9], df.columns[10], df.columns[11]\n",
    "(col1, col2, col3, col4, col5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db176d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F.regexp_replace(F.col(col5), data_regex, '').isNotNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d199d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple transform\n",
    "df = df.withColumn(col1, F.col(col1)*5)\n",
    "\n",
    "# using when to set two different values \n",
    "df = df.withColumn(col2, F.when(F.col(col2) == 1, True).otherwise(False))\n",
    "\n",
    "# only casting to float if we can bring the entire column\n",
    "row_ct = df.count()\n",
    "rowswith_col3float = df.filter(F.col(col3).cast(\"float\").isNotNull()).count()\n",
    "if row_ct == rowswith_col3float:\n",
    "    df = df.withColumn(col3, F.col(col3).cast(\"float\"))\n",
    "\n",
    "# using a regex to clean/remove anything non numerical in any string,\n",
    "# and then do the cast to float for non null (empty str) values\n",
    "df = df.withColumn(col3, F.regexp_replace(F.col(col3), r\"[^0-9\\\\.]\", '').cast(\"double\"))\n",
    "\n",
    "# using a custom function to convert col4 back to int\n",
    "# note the necessary redundant logic to process null\n",
    "def floatint(x):\n",
    "    return int(float(x))\n",
    "int_udf = F.udf(lambda m: None if m is None else floatint(m))\n",
    "df = df.withColumn(col4, F.when(F.col(col4).isNotNull(), int_udf(F.col(col4))).otherwise(None))\n",
    "# df = df.withColumn(col4, int_udf(F.col(col4))) # can also just do a udf without when\n",
    "\n",
    "# same, using a regex for matching dates\n",
    "data_regex = r\"\\d{2,4}(\\.|\\-|\\/|\\\\)+\\d{2,4}(\\.|\\-|\\/|\\\\)+\\d{2,4}(\\s)*(\\d{2}\\:\\d{2}\\:\\d{2})?(\\.\\d{3})?|^$\"\n",
    "df = df.withColumn(col5, F.when(F.regexp_replace(F.col(col5), data_regex, '').isNotNull(),\\\n",
    "                                F.to_timestamp(F.col(col5), 'yyyy/MM/dd')).otherwise(None))\n",
    "# df = df.withColumn(col5, F.to_timestamp(F.col(col5), 'yyyy/MM/dd')) # simpler alternative\n",
    "\n",
    "# replace and nan in the df with nulls\n",
    "df = df.replace(np.nan, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db55630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw|sYef|00L3k| OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "|       51|     10|       37|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       83|    440|       38|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       40|    150|       25|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       27|    260|       66|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       65|    215|       57|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       59|     70|       93|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       28|    420|       20|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       51|      5|       52|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       46|    180|       20|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       28|    100|       63|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       82|    465|       61|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       99|    285|       34|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       30|    145|       34|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       59|     10|       16|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       17|    125|       18|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        9|    440|        2|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|       54|    315|       90|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        1|     10|       73|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        3|    355|       63|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "|        9|    160|       57|     null|null|  null|  null|  null|   false|null| null|null|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1898b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "|SymxUUBCx|RcKB398|uNQMYeBmQ|lOihuqh9a|pIkq|21rkd6|yLdLOm|CLrQTb|LFThQKIw|sYef|00L3k|                OjQ|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "|     null|    150|       57|       49|  34|    83|    26|    43|   false|null|   50|2020-02-01 00:00:00|\n",
      "|     null|    410|       68|       92|  68|    43|    76|    61|    true|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|     40|       92|       51|  37|    23|    46|    86|    true|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|     65|        7|       89|  81|    52|    86|    87|   false|34.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    135|       65|       20|  37|    29|    73|    70|   false|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|    245|       56|       42|  88|    49|    79|    11|    true|null|   10|2020-02-01 00:00:00|\n",
      "|     null|    230|       46|       84|  75|    97|    92|    11|    true|86.0|   50|2020-02-01 00:00:00|\n",
      "|     null|     55|       28|       53|  99|    77|    21|    60|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|     45|       27|       89|  99|    57|    36|    59|    true|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    290|       53|       38|  21|    53|    71|    52|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    425|       67|       48|  19|    35|     1|    72|   false|23.0|   25|2020-02-01 00:00:00|\n",
      "|     null|    335|       92|       95|  31|    89|    86|    11|   false|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    230|        1|       93|  84|    59|    80|    31|    true|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|    335|       13|       40|  81|    21|    35|    24|   false|13.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    215|       63|       82|  87|    52|    29|    83|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    120|       98|       64|  38|    20|    25|    83|   false|23.0|   10|2020-02-01 00:00:00|\n",
      "|     null|     40|       54|       63|  47|     4|    35|    84|   false|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    345|       67|       82|  30|    96|    54|    25|    true|null|   50|2020-02-01 00:00:00|\n",
      "|     null|     95|       80|       68|  95|    62|    19|    80|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "|     null|    460|       38|       79|  24|    90|    40|    20|    true|23.0|   50|2020-02-01 00:00:00|\n",
      "+---------+-------+---------+---------+----+------+------+------+--------+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the last 20 rows\n",
    "df.orderBy(F.desc(\"index\")).drop(\"index\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b388cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SymxUUBCx', 'int'),\n",
       " ('RcKB398', 'int'),\n",
       " ('uNQMYeBmQ', 'int'),\n",
       " ('lOihuqh9a', 'int'),\n",
       " ('pIkq', 'int'),\n",
       " ('21rkd6', 'int'),\n",
       " ('yLdLOm', 'int'),\n",
       " ('CLrQTb', 'int'),\n",
       " ('LFThQKIw', 'boolean'),\n",
       " ('sYef', 'double'),\n",
       " ('00L3k', 'string'),\n",
       " ('OjQ', 'timestamp'),\n",
       " ('index', 'bigint')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27faf4",
   "metadata": {},
   "source": [
    "You can see the same kinds of transforms are avaialable in pyspark and you can do most of what you'd do with a dataframe. There's also ways to do this outside the dataframe api with a raw rdd but that's beyond the scope of what I wanted to show here.\n",
    "\n",
    "# This example was simple but your data problems may be complex.\n",
    "\n",
    "# For any of your complex data problems I'm available to hire on contract to help you build and scale whatever data decision engine you need for your business. \n",
    "\n",
    "# Reach out by messaging inquire@automatedinnovations.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a5990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
